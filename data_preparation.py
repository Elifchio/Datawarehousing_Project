#import needed librariesimport csvimport os#only for additional checks if neededimport pandas as pd#delete old variable if any#%reset -f os.getcwd()os.chdir('C:/Users/alix2/Desktop/dss PROJECT')path_to_use=os.getcwd()# defining a func to create copies to work on# use "with" when opening a file so theres no need to close manuallydef copy_file(path, original, copy):    """    Copies the content of "original" to copy, csv.    Parameters:    paths to files    """    with open(path+'/'+original, 'r') as original_file:        with open(copy, 'w') as copy_file:            for line in original_file:                copy_file.write(line)copy_file(path_to_use, 'Crashes.csv', 'Crashes1.csv')copy_file(path_to_use, 'Vehicles.csv', 'Vehicles1.csv')copy_file(path_to_use, 'People.csv', 'People1.csv')# creating pandas df in case they are neededcrashes_df = pd.read_csv('Crashes1.csv', skipinitialspace=True) vehicles_df = pd.read_csv('Vehicles1.csv', skipinitialspace=True) people_df = pd.read_csv('People1.csv', skipinitialspace=True) #first we check if every columns ' values has the same data type# checking datatypes, unique values and null counts# 1) functions for checking data types in columndef infer_type(value):    """    Infers the type of a value in a CSV file.    Returns one of 'int', 'float', 'str', or 'empty'.    """    try:        int(value)        return 'int'    except ValueError:        pass    try:        float(value)        return 'float'    except ValueError:        pass    if value.strip() == '':        return 'empty'    return 'str'  # default to string if none of the abovedef analyze_csv_types(file_path):    """    Analyzes the types of data present in each column of a CSV file and returns    a summary for each column in one line: (index, name, inferred types with counts).        Parameters:    - file_path (str): Path to the CSV file.    Returns:    - list: A list of strings summarizing the analysis for each column,             showing types and their respective counts.    """    with open(file_path, mode='r', newline='') as infile:        reader = csv.DictReader(infile)        column_names = reader.fieldnames        column_indices = {col: idx for idx, col in enumerate(column_names)}  # map column names to indices        column_type_counts = {col: {} for col in column_names}  # initialize type counts for each column        for row in reader:            for col, value in row.items():                inferred_type = infer_type(value)                # update type counts                if inferred_type not in column_type_counts[col]:                    column_type_counts[col][inferred_type] = 0                column_type_counts[col][inferred_type] += 1        # prepare summary for each column        column_summaries = []        for col in column_names:            index = column_indices[col]            type_counts = ', '.join(f"{t}: {c}" for t, c in column_type_counts[col].items())            total_entries = sum(column_type_counts[col].values())            summary = f"{index} {col} : {type_counts} : Total {total_entries}"            column_summaries.append(summary)    return column_summaries        # 2) function to turn nan into unknown (string), or -1 (for numerical values): def replace_nan_by_column_type(file_path):    """    Replaces NaN or empty values in a CSV file based on column type:    - Replaces with "UNKNOWN" for string-dominant columns.    - Replaces with -1 for numeric-dominant columns.    Parameters:        file_path (str): Path to the CSV file to be updated.    """    temp_file = "temp.csv"  #temporary file for intermediate updates    try:        #read the file to determine column types        with open(file_path, mode='r', newline='') as infile:            reader = csv.reader(infile)            header = next(reader)  #read the header row            rows = list(reader)   #read all rows into memory for analysis        #analyze column types        column_types = {}        for col_index, col_name in enumerate(header):            string_count = 0            numeric_count = 0            for row in rows:                cell = row[col_index].strip()                if cell == "" or cell == "UNKNOWN":                    continue                try:                    float(cell)  #check if the cell can be converted to a float                    numeric_count += 1                except ValueError:                    string_count += 1            #determine column type            column_types[col_index] = 'numeric' if numeric_count >= string_count else 'string'        #process the file again and replace NaN/UNKNOWN values        with open(file_path, mode='r', newline='') as infile, open(temp_file, mode='w', newline='') as outfile:            reader = csv.reader(infile)            writer = csv.writer(outfile)            # write the header once            header = next(reader)  # ensure we skip the header here too            writer.writerow(header)            # process and write each row            for row in reader:                updated_row = [                    "UNKNOWN" if (cell.strip() in ["", "UNKNOWN"] and column_types[col_index] == 'string') else                    -1 if (cell.strip() in ["", "UNKNOWN"] and column_types[col_index] == 'numeric') else                    cell                    for col_index, cell in enumerate(row)                ]                writer.writerow(updated_row)        #replace the original file with the temporary file        os.replace(temp_file, file_path)        print(f"Updated NaN/UNKNOWN values in {file_path} based on column types.")    except FileNotFoundError:        print(f"Error: File '{file_path}' not found.")    except Exception as e:        print(f"An error occurred: {e}")                # conversion between float-int for when necessarydef convert_column_floats_to_integers_inplace(file_path, column_name):    """    Reads a CSV file, converts the specified column's float values to integers,    and overwrites the file with the updated data.    Parameters:    - file_path (str): Path to the CSV file to modify.    - column_name (str): Name of the column to process.    """    #read the file into memory    with open(file_path, mode='r', newline='') as infile:        reader = csv.DictReader(infile)        fieldnames = reader.fieldnames  # get column names        rows = list(reader)  # read all rows into memory    #modify the specified column    for row in rows:        try:            # convert to float first, then to integer            row[column_name] = int(float(row[column_name].strip())) if row[column_name].strip() else None        except ValueError:            # handle invalid data gracefully            row[column_name] = None    # write the modified rows back to the file    with open(file_path, mode='w', newline='') as outfile:        writer = csv.DictWriter(outfile, fieldnames=fieldnames)        writer.writeheader()  # write the header row        writer.writerows(rows)  # write all modified rows    print(f"File '{file_path}' has been updated. Column '{column_name}' converted to integers.")def convert_column_integers_to_floats_inplace(file_path, column_name):    """    Reads a CSV file, converts the specified column's numeric values to floats,    and overwrites the file with the updated data.    Parameters:    - file_path (str): Path to the CSV file to modify.    - column_name (str): Name of the column to process.    """    #read the file into memory    with open(file_path, mode='r', newline='') as infile:        reader = csv.DictReader(infile)        fieldnames = reader.fieldnames  #get column names        rows = list(reader)  # read all rows into memory    #modify the specified column    for row in rows:        try:            #convert to float            row[column_name] = float(row[column_name].strip()) if row[column_name].strip() else None        except ValueError:            # handle invalid data gracefully            row[column_name] = None    # write the modified rows back to the file    with open(file_path, mode='w', newline='') as outfile:        writer = csv.DictWriter(outfile, fieldnames=fieldnames)        writer.writeheader()  # Write the header row        writer.writerows(rows)  # Write all modified rows    print(f"File '{file_path}' has been updated. Column '{column_name}' converted to floats.")# when there are several columns to do itdef convert_multiple_columns(file_path, column_names):    for column_name in column_names:        convert_column_floats_to_integers_inplace(file_path, column_name)        def convert_multiple_columns_float(file_path, column_names):    for column_name in column_names:        convert_column_integers_to_floats_inplace(file_path, column_name)        # date formatting# creating a function to make date entries sql server compatiblefrom datetime import datetimedef reformat_dates_for_sql(file_path, date_column_index):    """   formatting to datetime type compatible way        Parameters:    - file_path (str): Path to the CSV file to process.    - date_column_index (int): Zero-based index of the column containing the date.    """    input_date_format = '%m/%d/%Y %I:%M:%S %p'      output_date_format = '%Y-%m-%d %H:%M:%S'       temp_file = 'temp.csv'          with open(file_path, mode='r', newline='') as infile, open(temp_file, mode='w', newline='') as outfile:        reader = csv.reader(infile)        writer = csv.writer(outfile)                header = next(reader)        writer.writerow(header)                for row in reader:            # making sure that the row and date column are not empty            if row and row[date_column_index]:                try:                                        date_obj = datetime.strptime(row[date_column_index], input_date_format)                    row[date_column_index] = date_obj.strftime(output_date_format)                except ValueError as e:                    print(f"Error processing date in row: {row} - {e}")            writer.writerow(row)    os.replace(temp_file, file_path)    print(f"File '{file_path}' has been updated with SQL Server compatible dates.")    # unique value observationdef analyze_column(df, column_name):       unique_count = df[column_name].nunique()    value_counts = df[column_name].value_counts().to_dict()    total_entries = len(df[column_name])        return unique_count, value_counts, total_entries# observing values repeating under a given ratiodef count_values_below_threshold(df, column_name, threshold):        value_counts = df[column_name].value_counts()    below_threshold_count = value_counts[value_counts <= threshold].sum()    total_entries = len(df[column_name])    ratio = below_threshold_count / total_entries if total_entries > 0 else 0        return {        'total_values_below_threshold': below_threshold_count,        'total_entries': total_entries,        'ratio': ratio    }# PEOPLE# start by replacing nanreplace_nan_by_column_type("People1.csv")# types in columnsanalyze_csv_types("People1.csv")# setting integer city values to unknownfile_path = path_to_use+'/'+'People1.csv'  column_to_modify = 'CITY' def is_integer(value):    try:        int(value)          return True    except (ValueError, TypeError):         return Falsetemp_file = 'temp.csv'with open(file_path, mode='r', newline='') as infile, open(temp_file, mode='w', newline='') as outfile:    reader = csv.DictReader(infile)    if not reader.fieldnames:        print(f"Error: The file '{file_path}' is empty or missing a header row.")        exit()    fieldnames = reader.fieldnames      writer = csv.DictWriter(outfile, fieldnames=fieldnames)    writer.writeheader()          for row in reader:        value = row.get(column_to_modify)        if value is not None and value.strip() != '' and is_integer(value):              row[column_to_modify] = "UNKNOWN"          writer.writerow(row)  os.replace(temp_file, file_path)print(f"File '{file_path}' updated. Integer values in column '{column_to_modify}' were replaced with 'UNKNOWN'.")# Sex: Initially M,F U,X nan. Turning U and X to other, nan will be unknown later    column_to_modify = 'SEX'replacement_map = {    'U': 'OTHER',    'X': 'OTHER'}temp_file_path = 'temp.csv'#read, modify, and write the CSVwith open(file_path, mode='r', newline='') as infile, open(temp_file_path, mode='w', newline='') as outfile:    reader = csv.DictReader(infile)    fieldnames = reader.fieldnames      writer = csv.DictWriter(outfile, fieldnames=fieldnames)        writer.writeheader()          for row in reader:            if row[column_to_modify] in replacement_map:            row[column_to_modify] = replacement_map[row[column_to_modify]]        writer.writerow(row)  os.replace(temp_file_path, file_path)print(f"Values in column '{column_to_modify}' have been updated in '{file_path}'.")# fixing columns that should be integers instead of floatscolumns_to_convert = ['VEHICLE_ID', 'AGE']  convert_multiple_columns(file_path, columns_to_convert)#end of people#VEHICLES# starting with nanreplace_nan_by_column_type("Vehicles1.csv")vehicles_df = pd.read_csv('Vehicles1.csv', skipinitialspace=True) # types in columns#analyze_csv_types("Vehicles1.csv")# handling integer values in modeldef convert_numbers_to_strings_in_column(file_path, column_name):    temp_file_path = file_path + '.tmp'    with open(file_path, mode='r', newline='', encoding='utf-8') as infile, \         open(temp_file_path, mode='w', newline='', encoding='utf-8') as outfile:        reader = csv.DictReader(infile)        fieldnames = reader.fieldnames          writer = csv.DictWriter(outfile, fieldnames=fieldnames)                writer.writeheader()                  for row in reader:            if column_name in row:                  value = row[column_name]                try:                                        float(value)                      row[column_name] = f'"{value}"'                  except ValueError:                                        pass            writer.writerow(row)              import os    os.replace(temp_file_path, file_path)    convert_numbers_to_strings_in_column("Vehicles1.csv", "MODEL")# fixing crash_date formatreformat_dates_for_sql(    file_path='Vehicles1.csv',      date_column_index=2         )# fixing errors in vehichle_year#comparing crash date to vehicle_year and marking unknown if crash date < model year #make sure to run it after setting missing values to -1file_path = path_to_use+'/'+'Vehicles1.csv'temp_file = 'temp.csv'crash_date_index = 2  # 3rd column is CRASH_DATEvehicle_year_index = 9  # 10th column is VEHICLE_YEARwith open(file_path, mode='r', newline='') as infile, open(temp_file, mode='w', newline='') as outfile:    reader = csv.reader(infile)    writer = csv.writer(outfile)     header = next(reader)    writer.writerow(header)    for row in reader:        try:               crash_year = datetime.strptime(row[crash_date_index], '%Y-%m-%d %H:%M:%S').year                 vehicle_year = row[vehicle_year_index]            if vehicle_year == '-1':                  pass              elif vehicle_year.replace('.', '', 1).isdigit():                  if int(float(vehicle_year)) > crash_year:                      row[vehicle_year_index] = -1                else:                    row[vehicle_year_index] = str(int(float(vehicle_year)))          except ValueError as e:                    print(f"Error processing row: {row} - {e}")              writer.writerow([str(cell) for cell in row])  os.replace(temp_file, file_path)print(f"File '{file_path}' has been updated.")# creating others bins#Option 1:# defining a func to mark rare occurings as other VIA RATIOimport csvfrom collections import Counterdef group_rare_categories_by_ratio(file_path, column_index, ratio):    """       - file_path (str): The path to the CSV file to process (input and output file).    - column_index (int): The zero-based index of the column to process.    - ratio (float): The percentage (as a decimal) below which values are grouped into 'Other'.    """        with open(file_path, mode='r', newline='') as infile:        reader = csv.reader(infile)        rows = list(reader)      header = rows[0]      data_rows = rows[1:]          column_values = [row[column_index] for row in data_rows]    value_counts = Counter(column_values)    total_entries = len(column_values)        threshold_count = total_entries * ratio        rare_values = {value for value, count in value_counts.items() if count < threshold_count}        for row in data_rows:        if row[column_index] in rare_values:            row[column_index] = "OTHER"        with open(file_path, mode='w', newline='') as outfile:        writer = csv.writer(outfile)        writer.writerow(header)          writer.writerows(data_rows)      print(f"File '{file_path}' has been updated with rare categories grouped into 'OTHER'.")#Option 2:# defining a func to mark rare occurings as other VIA NUMBER OF OCCURANCEdef group_rare_categories_by_threshold(file_path, column_index, threshold):    """    - file_path (str): The path to the CSV file to process (input and output file).    - column_index (int): The zero-based index of the column to process.    - threshold (int): Minimum frequency a value must have to not be grouped into 'Other'.    """    with open(file_path, mode='r', newline='') as infile:        reader = csv.reader(infile)        rows = list(reader)      header = rows[0]     data_rows = rows[1:]      column_values = [row[column_index] for row in data_rows]    value_counts = Counter(column_values)      rare_values = {value for value, count in value_counts.items() if count < threshold}     for row in data_rows:        if row[column_index] in rare_values:            row[column_index] = "Other"    with open(file_path, mode='w', newline='') as outfile:        writer = csv.writer(outfile)        writer.writerow(header)  # write the header         writer.writerows(data_rows)  # write the modified data rows    print(f"File '{file_path}' has been updated with rare categories grouped into 'Other'.")#MAKE#analyze_column(vehicles_df, 'MAKE')count_values_below_threshold(vehicles_df, 'MAKE', 2)# Grouping rare occurings of make as othergroup_rare_categories_by_ratio("Vehicles1.csv", 6, 0.005)# MODEL# analyze_column(vehicles_df, 'MODEL')count_values_below_threshold(vehicles_df, 'MODEL', 50)# grouping rare occurings of model as other - thresholdgroup_rare_categories_by_threshold("Vehicles1.csv", 7, 50)# fixing what should be integer instead of floatscolumns_to_convert = ['VEHICLE_ID', 'OCCUPANT_CNT']  convert_multiple_columns(file_path, columns_to_convert)# end of vehicle# CRASHES# dropping injuries unknown, which is always 0 because it is unknownfile_path =  path_to_use+'/'+'Crashes1.csv'  column_to_drop = 'INJURIES_UNKNOWN'  with open(file_path, mode='r', newline='') as infile:    reader = csv.DictReader(infile)      fieldnames = [col for col in reader.fieldnames if col != column_to_drop]             rows = []    for row in reader:        row.pop(column_to_drop, None)          rows.append(row)with open(file_path, mode='w', newline='') as outfile:    writer = csv.DictWriter(outfile, fieldnames=fieldnames)      writer.writeheader()      writer.writerows(rows)  print(f"Column '{column_to_drop}' has been dropped. Updated {file_path}.")#Latitude and longitude# first checking if the range is accurateprint(crashes_df["LATITUDE"].describe())print(crashes_df["LONGITUDE"].describe())#seems accuratedef round_column_in_csv(file_path, columns_to_round, decimals=5):      temp_file_path = file_path + '.tmp'        with open(file_path, mode='r', newline='', encoding='utf-8') as infile:        reader = csv.DictReader(infile)        fieldnames = reader.fieldnames        with open(temp_file_path, mode='w', newline='', encoding='utf-8') as tempfile:            writer = csv.DictWriter(tempfile, fieldnames=fieldnames)            writer.writeheader()            for row in reader:                for column in columns_to_round:                    if column in row:                        try:                            row[column] = f"{round(float(row[column]), decimals):.{decimals}f}"                        except ValueError:                            pass                writer.writerow(row)    os.replace(temp_file_path, file_path)    print(f"Rounded values updated in {file_path} for columns: {columns_to_round}")file_path = 'Crashes1.csv'columns_to_round = ['LONGITUDE', 'LATITUDE']  round_column_in_csv(file_path, columns_to_round, decimals=5)file_path = 'Crashes1.csv'columns_to_round = ['LONGITUDE', 'LATITUDE']  convert_multiple_columns_float(file_path, columns_to_round)# formatting 2 date columns# crash datereformat_dates_for_sql(    file_path='Crashes1.csv',      date_column_index=1         )# DATE_POLICE_NOTIFIEDreformat_dates_for_sql(    file_path='Crashes1.csv',      date_column_index=14         )# handling floats that should be integersfile_path = 'Crashes1.csv'columns_to_convert = ['NUM_UNITS', 'INJURIES_TOTAL', "INJURIES_FATAL", "INJURIES_INCAPACITATING", "INJURIES_NON_INCAPACITATING",                      "INJURIES_REPORTED_NOT_EVIDENT", "INJURIES_NO_INDICATION", "BEAT_OF_OCCURRENCE"]  convert_multiple_columns(file_path, columns_to_convert)# now that also beat of occurence is inevitable, we can imputereplace_nan_by_column_type("Crashes1.csv")